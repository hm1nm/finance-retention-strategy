{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f373542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# ========================================================================================\n",
    "# 1. Configuration & Constants\n",
    "# ========================================================================================\n",
    "\n",
    "# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï\n",
    "if platform.system() == 'Darwin': # Mac\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "else: # Windows\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "COL_ID = 'Î∞úÍ∏âÌöåÏõêÎ≤àÌò∏'\n",
    "COL_DATE = 'Í∏∞Ï§ÄÎÖÑÏõî'\n",
    "\n",
    "# [ÏàòÏ†ïÎê®] Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ Î™ÖÏÑ∏ÏÑú(ÌïÑÎìúÌïúÍ∏ÄÎ™Ö) Í∏∞Î∞ò Îß§Ìïë\n",
    "COL_SPEND = 'Ïù¥Ïö©Í∏àÏï°_Ïã†Ïö©_B0M'\n",
    "COL_COUNT = 'Ïù¥Ïö©Í±¥Ïàò_Ïã†Ïö©_B0M'\n",
    "COL_BALANCE = 'ÏûîÏï°_B0M'\n",
    "COL_AVG_BAL = 'ÌèâÏûî_3M'\n",
    "COL_CASH_ADV = 'Ïù¥Ïö©Í∏àÏï°_CA_B0M'\n",
    "COL_CARD_LOAN = 'Ïù¥Ïö©Í∏àÏï°_Ïπ¥ÎìúÎ°†_B0M'\n",
    "COL_DELINQ = 'ÌöåÏõêÏó¨Î∂Ä_Ïó∞Ï≤¥'\n",
    "COL_SPEND_R12M = 'Ïù¥Ïö©Í∏àÏï°_Ïã†Ïö©_R12M'\n",
    "COL_COUNT_R12M = 'Ïù¥Ïö©Í±¥Ïàò_Ïã†Ïö©_R12M'\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================================================================================\n",
    "# 2. Utility Functions\n",
    "# ========================================================================================\n",
    "def calc_slope_long(series):\n",
    "    y = series.values\n",
    "    n = len(y)\n",
    "    if n < 2: return 0.0\n",
    "    x = np.arange(n)\n",
    "    if np.all(y == y[0]): return 0.0\n",
    "    slope, _, _, _, _ = linregress(x, y)\n",
    "    if np.isnan(slope): return 0.0\n",
    "    return slope\n",
    "\n",
    "def normalize_risk_vector(series):\n",
    "    if series.empty: return series\n",
    "    risk_raw = -series\n",
    "    min_val = risk_raw.min()\n",
    "    max_val = risk_raw.max()\n",
    "    if max_val == min_val: return pd.Series(0, index=series.index)\n",
    "    normalized = (risk_raw - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def calculate_vif(dataframe, sample_size=5000):\n",
    "    print(\"\\nüîç [VIF Check] Calculating Variance Inflation Factors...\")\n",
    "    df_vif_input = dataframe.select_dtypes(include=[np.number]).dropna()\n",
    "    if len(df_vif_input) > sample_size:\n",
    "        df_vif_input = df_vif_input.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    cols_to_exclude = ['Target', 'Î∞úÍ∏âÌöåÏõêÎ≤àÌò∏', 'Unnamed: 0', 'index']\n",
    "    cols_check = [c for c in df_vif_input.columns if c not in cols_to_exclude]\n",
    "    df_vif_input = df_vif_input[cols_check]\n",
    "    df_vif_input = add_constant(df_vif_input)\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df_vif_input.columns\n",
    "    try:\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(df_vif_input.values, i) for i in range(df_vif_input.shape[1])]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è VIF calculation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    vif_data = vif_data[vif_data['Feature'] != 'const'].sort_values(by=\"VIF\", ascending=False)\n",
    "    print(vif_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"VIF\", y=\"Feature\", data=vif_data.head(20))\n",
    "    plt.title(\"Top 20 Features by VIF\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return vif_data\n",
    "\n",
    "# ========================================================================================\n",
    "# 3. Core Logic: Scoring & Target Generation\n",
    "# ========================================================================================\n",
    "def calculate_churn_scores(group):\n",
    "    # (Same logic as existing pipelines)\n",
    "    months_data = len(group)\n",
    "    if months_data >= 2:\n",
    "        slope_spend = calc_slope_long(group[COL_SPEND])\n",
    "        slope_balance = calc_slope_long(group[COL_BALANCE])\n",
    "        slope_count = calc_slope_long(group[COL_COUNT])\n",
    "    else:\n",
    "        current_spend = group[COL_SPEND].iloc[-1]\n",
    "        r12m_spend = group[COL_SPEND_R12M].iloc[-1] if COL_SPEND_R12M in group.columns else 0\n",
    "        slope_spend = current_spend - r12m_spend\n",
    "        current_count = group[COL_COUNT].iloc[-1]\n",
    "        r12m_count = group[COL_COUNT_R12M].iloc[-1] if COL_COUNT_R12M in group.columns else 0\n",
    "        slope_count = current_count - r12m_count\n",
    "        slope_balance = -1 \n",
    "        \n",
    "    score_status_total = 0\n",
    "    delinq_sum = group[COL_DELINQ].sum() if COL_DELINQ in group.columns else 0\n",
    "    cash_adv_sum = group[COL_CASH_ADV].sum() if COL_CASH_ADV in group.columns else 0\n",
    "\n",
    "    if delinq_sum > 0: score_status_total += 50\n",
    "    if cash_adv_sum > 0: score_status_total += 30\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Score_Status_Total': score_status_total,\n",
    "        'Slope_Spend': slope_spend,\n",
    "        'Slope_Balance': slope_balance,\n",
    "        'Slope_Count': slope_count,\n",
    "        'Score_BadDebt': 1 if delinq_sum > 0 else 0, \n",
    "        'Score_Delinq': 1 if delinq_sum > 0 else 0,\n",
    "        'Score_Activity': -1 if slope_count < 0 else 0,\n",
    "        'Score_Asset': 0 \n",
    "    })\n",
    "\n",
    "def check_churn_condition(df_scores):\n",
    "    print(\" - Generating Target Variable...\")\n",
    "    cond_slopes_decrease = ((df_scores['Slope_Spend'] <= 0) & (df_scores['Slope_Balance'] <= 0) & (df_scores['Slope_Count'] <= 0))\n",
    "    cond1 = df_scores['Score_BadDebt'] > 0\n",
    "    cond2 = df_scores['Score_Delinq'] > 0\n",
    "    cond3 = df_scores['Score_Activity'] < 0\n",
    "    cond4 = df_scores['Score_Asset'] == 0 \n",
    "    risk_count = cond1.astype(int) + cond2.astype(int) + cond3.astype(int) + cond4.astype(int)\n",
    "    cond_high_risk = (risk_count >= 1)\n",
    "    \n",
    "    df_scores['Target'] = np.where(cond_slopes_decrease & cond_high_risk, 1, 0)\n",
    "    \n",
    "    norm_slope_spend = normalize_risk_vector(df_scores['Slope_Spend']) * 30\n",
    "    norm_slope_count = normalize_risk_vector(df_scores['Slope_Count']) * 30\n",
    "    norm_slope_bal = normalize_risk_vector(df_scores['Slope_Balance']) * 40 \n",
    "    \n",
    "    df_scores['Score_Slope_Total'] = norm_slope_spend + norm_slope_count + norm_slope_bal\n",
    "    df_scores['Final_Total_Score'] = (df_scores['Score_Status_Total'] + df_scores['Score_Slope_Total']) * 0.5\n",
    "    return df_scores\n",
    "\n",
    "def process_data_and_merge(file_path):\n",
    "    print(f\"\\n1. [Data Load] Loading {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Load failed: {e}\")\n",
    "        return None\n",
    "        \n",
    "    if COL_SPEND_R12M not in df.columns: df[COL_SPEND_R12M] = 0\n",
    "    if COL_COUNT_R12M not in df.columns: df[COL_COUNT_R12M] = 0\n",
    "\n",
    "    print(\" - Sorting data...\")\n",
    "    df.sort_values(by=[COL_ID, COL_DATE], inplace=True)\n",
    "    \n",
    "    print(\"2. [Scoring] Calculating Churn Scores...\")\n",
    "    try:\n",
    "        df_scores = df.groupby(COL_ID).apply(calculate_churn_scores).reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Grouping Error: {e}\")\n",
    "        return None\n",
    "        \n",
    "    df_scores = check_churn_condition(df_scores)\n",
    "    print(f\" - Target Ratio: {df_scores['Target'].value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    print(\"3. [Merge] Merging Scores with Features...\")\n",
    "    df_last = df.groupby(COL_ID).tail(1).copy()\n",
    "    df_final = df_last.merge(df_scores, on=COL_ID, how='left')\n",
    "    return df_final\n",
    "\n",
    "def plot_score_distributions(df, target_col='Target'):\n",
    "    print(\"\\nüìä [Distribution Analysis] Plotting Score Distributions...\")\n",
    "    cols = ['Final_Total_Score', 'Score_Slope_Total', 'Score_Status_Total', 'Slope_Spend', 'Slope_Count', 'Slope_Balance']\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols: return\n",
    "    fig, axes = plt.subplots(nrows=len(cols), ncols=2, figsize=(15, 4 * len(cols)))\n",
    "    for i, col in enumerate(cols):\n",
    "        sns.histplot(data=df, x=col, hue=target_col, kde=True, element=\"step\", ax=axes[i, 0], palette='Set1')\n",
    "        axes[i, 0].set_title(f'{col} Distribution by Target')\n",
    "        sns.boxplot(data=df, x=target_col, y=col, ax=axes[i, 1], palette='Set1')\n",
    "        axes[i, 1].set_title(f'{col} Boxplot by Target')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========================================================================================\n",
    "# 4. FT-Transformer Implementation\n",
    "# ========================================================================================\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32) if y is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    def __init__(self, num_numerical_features, d_token):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(num_numerical_features, d_token))\n",
    "        self.bias = nn.Parameter(torch.randn(num_numerical_features, d_token))\n",
    "        \n",
    "    def forward(self, x_num):\n",
    "        # x_num: (batch_size, num_numerical_features)\n",
    "        # out: (batch_size, num_numerical_features, d_token)\n",
    "        x = x_num.unsqueeze(-1) * self.weight.unsqueeze(0) + self.bias.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, num_numerical_features, d_token=192, n_layers=3, n_heads=8, d_ffn=None, attention_dropout=0.1, ffn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_ffn is None:\n",
    "            d_ffn = d_token * 4 // 3\n",
    "            \n",
    "        self.tokenizer = FeatureTokenizer(num_numerical_features, d_token)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token, \n",
    "            nhead=n_heads, \n",
    "            dim_feedforward=d_ffn, \n",
    "            dropout=attention_dropout, \n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_num):\n",
    "        # x_num: (batch_size, num_features)\n",
    "        batch_size = x_num.shape[0]\n",
    "        \n",
    "        # Tokenize (Embeddings)\n",
    "        x = self.tokenizer(x_num) # (B, F, D)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # (B, 1, D)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (B, F+1, D)\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Use CLS token for prediction\n",
    "        x_cls = x[:, 0, :]\n",
    "        logits = self.head(x_cls)\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "# ========================================================================================\n",
    "# 5. Training Logic\n",
    "# ========================================================================================\n",
    "\n",
    "def train_eval_fttransformer(df_train, target_col='Target'):\n",
    "    print(f\"\\nüèãÔ∏è [Model Training] FT-Transformer (PyTorch)...\")\n",
    "    \n",
    "    # Feature Selection (Removing Leakage)\n",
    "    leakage_cols = [\n",
    "        target_col, COL_ID, 'Unnamed: 0',\n",
    "        'Slope_Spend', 'Slope_Balance', 'Slope_Count',\n",
    "        'Score_BadDebt', 'Score_Delinq', 'Score_Activity', 'Score_Asset',\n",
    "        'Score_Status_Total', 'Score_Slope_Total', 'Final_Total_Score'\n",
    "    ]\n",
    "    \n",
    "    continuous_features = [c for c in df_train.columns if c not in leakage_cols and pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "    \n",
    "    print(f\" - Using {len(continuous_features)} numerical features.\")\n",
    "    \n",
    "    X = df_train[continuous_features]\n",
    "    y = df_train[target_col]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Scaling (Crucial for Neural Networks)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=continuous_features)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=continuous_features)\n",
    "    \n",
    "    # Dataset & DataLoader\n",
    "    train_dataset = TabularDataset(X_train, y_train)\n",
    "    test_dataset = TabularDataset(X_test, y_test)\n",
    "    \n",
    "    batch_size = 256\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model Setup\n",
    "    model = FTTransformer(\n",
    "        num_numerical_features=len(continuous_features),\n",
    "        d_token=192,\n",
    "        n_layers=3,\n",
    "        n_heads=8,\n",
    "        attention_dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Class Weight needed for imbalanced data\n",
    "    pos_count = y_train.sum()\n",
    "    neg_count = len(y_train) - pos_count\n",
    "    pos_weight = torch.tensor([neg_count / pos_count], device=device) if pos_count > 0 else torch.tensor([1.0], device=device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Training Loop\n",
    "    epochs = 30\n",
    "    best_auc = 0\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    \n",
    "    print(f\" - Starting Training for {epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                all_preds.extend(probs.cpu().numpy())\n",
    "                all_targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_auc = roc_auc_score(all_targets, all_preds)\n",
    "        print(f\"   Epoch {epoch+1}/{epochs} | Loss: {train_loss/len(train_loader):.4f} | Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            torch.save(model.state_dict(), 'best_ft_transformer.pth')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"   Early stopping triggered.\")\n",
    "                break\n",
    "                \n",
    "    # Final Evaluation\n",
    "    model.load_state_dict(torch.load('best_ft_transformer.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred_probs = []\n",
    "    y_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            y_pred_probs.extend(probs.cpu().numpy())\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            \n",
    "    y_pred_probs = np.array(y_pred_probs)\n",
    "    y_pred_labels = (y_pred_probs >= 0.5).astype(int)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    print(\"\\n[FT-Transformer Results]\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred_labels):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred_labels):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_true, y_pred_probs):.4f}\")\n",
    "    print(classification_report(y_true, y_pred_labels))\n",
    "    \n",
    "    # Visualizations\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    cm = confusion_matrix(y_true, y_pred_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - FT-Transformer')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Permutation Importance (Simple)\n",
    "    print(\" - Calculating Permutation Importance...\")\n",
    "    calculate_permutation_importance(model, X_test, y_true, device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_permutation_importance(model, X_df, y_true, device):\n",
    "    \"\"\"\n",
    "    Calculate simple permutation importance for top features.\n",
    "    \"\"\"\n",
    "    baseline_auc = roc_auc_score(y_true, predict_proba(model, X_df, device))\n",
    "    importances = {}\n",
    "    \n",
    "    # Check top features by variance or simple correlation to speed up if needed\n",
    "    # checking all for now\n",
    "    features = X_df.columns\n",
    "    \n",
    "    for col in features:\n",
    "        save = X_df[col].copy()\n",
    "        X_df[col] = np.random.permutation(X_df[col])\n",
    "        \n",
    "        permuted_auc = roc_auc_score(y_true, predict_proba(model, X_df, device))\n",
    "        importances[col] = baseline_auc - permuted_auc\n",
    "        \n",
    "        X_df[col] = save # restore\n",
    "        \n",
    "    imp_df = pd.DataFrame(list(importances.items()), columns=['Feature', 'Importance'])\n",
    "    imp_df = imp_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=imp_df, palette='viridis')\n",
    "    plt.title('Permutation Feature Importance (FT-Transformer)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_proba(model, X_df, device):\n",
    "    X_tensor = torch.tensor(X_df.values, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# ========================================================================================\n",
    "# 6. Main Execution\n",
    "# ========================================================================================\n",
    "DATA_FILE_PATH = './260108/general_combined_part1.csv'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Feature Selection & Visualization Pipeline (FT-Transformer Version)...\")\n",
    "    \n",
    "    if os.path.exists(DATA_FILE_PATH):\n",
    "        df_final = process_data_and_merge(DATA_FILE_PATH)\n",
    "        \n",
    "        if df_final is not None:\n",
    "            # VIF and Distribution check\n",
    "            plot_score_distributions(df_final)\n",
    "            # define train_eval_fttransformer\n",
    "            model = train_eval_fttransformer(df_final)\n",
    "            \n",
    "            print(\"‚úÖ Pipeline (FT-Transformer) Completed Successfully.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Data file not found. Please check DATA_FILE_PATH.\")\n",
    "        print(f\"Current Pwd: {os.getcwd()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
