{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbbde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant  # VIF ìƒìˆ˜í•­ ì¶”ê°€ìš©\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import platform # OS í™•ì¸ìš©\n",
    "\n",
    "# ========================================================================================\n",
    "# 1. Configuration & Constants\n",
    "# ========================================================================================\n",
    "import platform\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "if platform.system() == 'Darwin': # Mac\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "else: # Windows\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "COL_ID = 'ë°œê¸‰íšŒì›ë²ˆí˜¸'\n",
    "COL_DATE = 'ê¸°ì¤€ë…„ì›”'\n",
    "\n",
    "# [ìˆ˜ì •ë¨] ì‹¤ì œ ë°ì´í„° ëª…ì„¸ì„œ(í•„ë“œí•œê¸€ëª…) ê¸°ë°˜ ë§¤í•‘\n",
    "# 3. ìŠ¹ì¸ë§¤ì¶œ ì •ë³´.csv ì°¸ê³ \n",
    "COL_SPEND = 'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M'       # ê¸°ì¡´: ë‹¹ì›”_ì´_ì´ìš©ê¸ˆì•¡ -> ë³€ê²½: ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M (ì¼ì‹œë¶ˆ+í• ë¶€+í˜„ê¸ˆì„œë¹„ìŠ¤+ì¹´ë“œë¡ )\n",
    "COL_COUNT = 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M'       # ê¸°ì¡´: ë‹¹ì›”_ì´_ì´ìš©ê±´ìˆ˜ -> ë³€ê²½: ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M\n",
    "\n",
    "# 5. ì”ì•¡ ì •ë³´.csv ì°¸ê³ \n",
    "COL_BALANCE = 'ì”ì•¡_B0M'             # ê¸°ì¡´: ë‹¹ì›”_ì‹ ìš©ê³µì—¬_ì´_ì”ì•¡ -> ë³€ê²½: ì”ì•¡_B0M\n",
    "COL_AVG_BAL = 'í‰ì”_3M'              # ê¸°ì¡´: ìµœê·¼_3ê°œì›”_í‰ê· _ì”ì•¡ -> ë³€ê²½: í‰ì”_3M\n",
    "\n",
    "# 3. ìŠ¹ì¸ë§¤ì¶œ ì •ë³´.csv ì°¸ê³ \n",
    "COL_CASH_ADV = 'ì´ìš©ê¸ˆì•¡_CA_B0M'      # ê¸°ì¡´: ë‹¹ì›”_í˜„ê¸ˆì„œë¹„ìŠ¤_ì´ìš©ê¸ˆì•¡ -> ë³€ê²½: ì´ìš©ê¸ˆì•¡_CA_B0M\n",
    "COL_CARD_LOAN = 'ì´ìš©ê¸ˆì•¡_ì¹´ë“œë¡ _B0M'  # ê¸°ì¡´: ë‹¹ì›”_ì¹´ë“œë¡ _ì´ìš©ê¸ˆì•¡ -> ë³€ê²½: ì´ìš©ê¸ˆì•¡_ì¹´ë“œë¡ _B0M\n",
    "\n",
    "# 1. íšŒì› ì •ë³´.csv ì°¸ê³ \n",
    "COL_DELINQ = 'íšŒì›ì—¬ë¶€_ì—°ì²´'          # ê¸°ì¡´: ë‹¹ì›”_ì—°ì²´_ì—¬ë¶€ -> ë³€ê²½: íšŒì›ì—¬ë¶€_ì—°ì²´ (0:ë¯¸ì—°ì²´, 1:ì—°ì²´)\n",
    "\n",
    "# Rolling 12 Months (R12M) Columns (1ë…„ì¹˜ ë°ì´í„°ê°€ ìˆì„ ê²½ìš° ì‚¬ìš©)\n",
    "# ëª…ì„¸ì„œì— í•´ë‹¹ ì»¬ëŸ¼ë“¤ì´ ì¡´ì¬í•˜ë¯€ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.\n",
    "COL_SPEND_R12M = 'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R12M' # ìµœê·¼ 1ë…„ê°„ ì´ìš©ê¸ˆì•¡\n",
    "COL_COUNT_R12M = 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_R12M' # ìµœê·¼ 1ë…„ê°„ ì´ìš©ê±´ìˆ˜\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# 2. Utility Functions\n",
    "# ========================================================================================\n",
    "def calc_slope_long(series):\n",
    "    \"\"\"\n",
    "    Calculate the slope of a linear regression line for a given series.\n",
    "    Returns 0 if the series has fewer than 2 data points or variance is zero.\n",
    "    \"\"\"\n",
    "    y = series.values\n",
    "    n = len(y)\n",
    "    if n < 2:\n",
    "        return 0.0\n",
    "    x = np.arange(n)\n",
    "    if np.all(y == y[0]):\n",
    "        return 0.0\n",
    "    slope, _, _, _, _ = linregress(x, y)\n",
    "    if np.isnan(slope):\n",
    "        return 0.0\n",
    "    return slope\n",
    "\n",
    "def normalize_risk_vector(series):\n",
    "    \"\"\"\n",
    "    Normalize slope values to a risk score (0-1).\n",
    "    Negative slope (decreasing trend) -> Higher Risk (closer to 1).\n",
    "    Positive slope (increasing trend) -> Lower Risk (closer to 0).\n",
    "    \"\"\"\n",
    "    if series.empty:\n",
    "        return series\n",
    "    \n",
    "    # We want decreasing trend (negative slope) to be high risk.\n",
    "    # So we inverse the values: risk_raw = -slope\n",
    "    risk_raw = -series\n",
    "    \n",
    "    # Min-Max Scaling to 0~1\n",
    "    min_val = risk_raw.min()\n",
    "    max_val = risk_raw.max()\n",
    "    \n",
    "    if max_val == min_val:\n",
    "        return pd.Series(0, index=series.index)\n",
    "        \n",
    "    normalized = (risk_raw - min_val) / (max_val - min_val)\n",
    "    return normalized\n",
    "\n",
    "def calculate_vif(dataframe, sample_size=5000):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) to detect multicollinearity.\n",
    "    Samples data if it's too large. Adds constant for correct calculation.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” [VIF Check] Calculating Variance Inflation Factors...\")\n",
    "    \n",
    "    df_vif_input = dataframe.select_dtypes(include=[np.number]).dropna()\n",
    "    \n",
    "    # Sampling if data is large\n",
    "    if len(df_vif_input) > sample_size:\n",
    "        print(f\" - Data size ({len(df_vif_input)}) is large. Sampling {sample_size} rows for VIF calculation.\")\n",
    "        df_vif_input = df_vif_input.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "    # Remove leakage/target columns if present\n",
    "    cols_to_exclude = ['Target', 'ë°œê¸‰íšŒì›ë²ˆí˜¸', 'Unnamed: 0', 'index']\n",
    "    cols_check = [c for c in df_vif_input.columns if c not in cols_to_exclude]\n",
    "    df_vif_input = df_vif_input[cols_check]\n",
    "    \n",
    "    # [ìˆ˜ì •] VIF ê³„ì‚° ì „ ìƒìˆ˜í•­ ì¶”ê°€ (í•„ìˆ˜)\n",
    "    df_vif_input = add_constant(df_vif_input)\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = df_vif_input.columns\n",
    "    \n",
    "    try:\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(df_vif_input.values, i) \n",
    "                            for i in range(df_vif_input.shape[1])]\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ VIF calculation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ìƒìˆ˜í•­(const) í–‰ ì œê±° í›„ ì •ë ¬\n",
    "    vif_data = vif_data[vif_data['Feature'] != 'const']\n",
    "    vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "    print(vif_data)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"VIF\", y=\"Feature\", data=vif_data.head(20))\n",
    "    plt.title(\"Top 20 Features by VIF\")\n",
    "    plt.xlabel(\"VIF Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return vif_data\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# 3. Core Logic: Scoring & Target Generation\n",
    "# ========================================================================================\n",
    "\n",
    "def calculate_churn_scores(group):\n",
    "    \"\"\"\n",
    "    Calculate Churn Scores for a customer group (sorted by date).\n",
    "    Handles R12M fallback for customers with only 1 month of data.\n",
    "    \"\"\"\n",
    "    months_data = len(group)\n",
    "    \n",
    "    # --- Slope Calculation Logic ---\n",
    "    if months_data >= 2:\n",
    "        # Normal Slope Calculation\n",
    "        slope_spend = calc_slope_long(group[COL_SPEND])\n",
    "        slope_balance = calc_slope_long(group[COL_BALANCE])\n",
    "        slope_count = calc_slope_long(group[COL_COUNT])\n",
    "    else:\n",
    "        # Fallback for 1-month data using R12M\n",
    "        current_spend = group[COL_SPEND].iloc[-1]\n",
    "        r12m_spend = group[COL_SPEND_R12M].iloc[-1] if COL_SPEND_R12M in group.columns else 0\n",
    "        slope_spend = current_spend - r12m_spend\n",
    "        \n",
    "        current_count = group[COL_COUNT].iloc[-1]\n",
    "        r12m_count = group[COL_COUNT_R12M].iloc[-1] if COL_COUNT_R12M in group.columns else 0\n",
    "        slope_count = current_count - r12m_count\n",
    "        \n",
    "        # Balance Slope Fallback -> Set to -1 (Risk) manually as per analysis\n",
    "        slope_balance = -1 \n",
    "        \n",
    "    score_status_total = 0\n",
    "    \n",
    "    # ì˜ˆì™¸ ì²˜ë¦¬: ì»¬ëŸ¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ getìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°\n",
    "    delinq_sum = group[COL_DELINQ].sum() if COL_DELINQ in group.columns else 0\n",
    "    cash_adv_sum = group[COL_CASH_ADV].sum() if COL_CASH_ADV in group.columns else 0\n",
    "\n",
    "    if delinq_sum > 0:\n",
    "        score_status_total += 50\n",
    "        \n",
    "    if cash_adv_sum > 0:\n",
    "        score_status_total += 30\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Score_Status_Total': score_status_total,\n",
    "        'Slope_Spend': slope_spend,\n",
    "        'Slope_Balance': slope_balance,\n",
    "        'Slope_Count': slope_count,\n",
    "        'Score_BadDebt': 1 if delinq_sum > 0 else 0, \n",
    "        'Score_Delinq': 1 if delinq_sum > 0 else 0,\n",
    "        'Score_Activity': -1 if slope_count < 0 else 0,\n",
    "        'Score_Asset': 0 \n",
    "    })\n",
    "\n",
    "def check_churn_condition(df_scores):\n",
    "    \"\"\"\n",
    "    Generate 'Target' based on scores.\n",
    "    \"\"\"\n",
    "    print(\" - Generating Target Variable...\")\n",
    "    \n",
    "    # 1. Slope Condition (Decreasing Trend)\n",
    "    cond_slopes_decrease = (\n",
    "        (df_scores['Slope_Spend'] <= 0) & \n",
    "        (df_scores['Slope_Balance'] <= 0) & \n",
    "        (df_scores['Slope_Count'] <= 0)\n",
    "    )\n",
    "    \n",
    "    # 2. Risk Count Condition\n",
    "    cond1 = df_scores['Score_BadDebt'] > 0\n",
    "    cond2 = df_scores['Score_Delinq'] > 0\n",
    "    cond3 = df_scores['Score_Activity'] < 0\n",
    "    cond4 = df_scores['Score_Asset'] == 0 \n",
    "    \n",
    "    risk_count = cond1.astype(int) + cond2.astype(int) + cond3.astype(int) + cond4.astype(int)\n",
    "    cond_high_risk = (risk_count >= 1)\n",
    "    \n",
    "    # Final Target\n",
    "    df_scores['Target'] = np.where(cond_slopes_decrease & cond_high_risk, 1, 0)\n",
    "    \n",
    "    # Calculate Total Scores for Analysis\n",
    "    norm_slope_spend = normalize_risk_vector(df_scores['Slope_Spend']) * 30\n",
    "    norm_slope_count = normalize_risk_vector(df_scores['Slope_Count']) * 30\n",
    "    norm_slope_bal = normalize_risk_vector(df_scores['Slope_Balance']) * 40 \n",
    "    \n",
    "    df_scores['Score_Slope_Total'] = norm_slope_spend + norm_slope_count + norm_slope_bal\n",
    "    df_scores['Final_Total_Score'] = (df_scores['Score_Status_Total'] + df_scores['Score_Slope_Total']) * 0.5\n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "def process_data_and_merge(file_path):\n",
    "    print(f\"\\n1. [Data Load] Loading {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # í•œê¸€ ê²½ë¡œ ì—ëŸ¬ ë°©ì§€ìš© engine='python'\n",
    "        df = pd.read_csv(file_path, low_memory=False) # engine='python'\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Load failed: {e}\")\n",
    "        return None\n",
    "        \n",
    "    if COL_SPEND_R12M not in df.columns: df[COL_SPEND_R12M] = 0\n",
    "    if COL_COUNT_R12M not in df.columns: df[COL_COUNT_R12M] = 0\n",
    "\n",
    "    print(\" - Sorting data...\")\n",
    "    df.sort_values(by=[COL_ID, COL_DATE], inplace=True)\n",
    "    \n",
    "    print(\"2. [Scoring] Calculating Churn Scores...\")\n",
    "    # pandas ìµœì‹  ë²„ì „ ëŒ€ì‘ (include_groups=False ê¶Œì¥ë˜ë‚˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ ì—ëŸ¬ì‹œ ìˆ˜ì •)\n",
    "    try:\n",
    "        df_scores = df.groupby(COL_ID).apply(calculate_churn_scores).reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"Grouping Error: {e}\")\n",
    "        return None\n",
    "        \n",
    "    df_scores = check_churn_condition(df_scores)\n",
    "    \n",
    "    print(f\" - Target Ratio: {df_scores['Target'].value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    print(\"3. [Merge] Merging Scores with Features...\")\n",
    "    df_last = df.groupby(COL_ID).tail(1).copy()\n",
    "    \n",
    "    df_final = df_last.merge(df_scores, on=COL_ID, how='left')\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# 4. Visualization Modules\n",
    "# ========================================================================================\n",
    "\n",
    "def plot_score_distributions(df, target_col='Target'):\n",
    "    print(\"\\nğŸ“Š [Distribution Analysis] Plotting Score Distributions...\")\n",
    "    \n",
    "    cols_to_plot = ['Final_Total_Score', 'Score_Slope_Total', 'Score_Status_Total', \n",
    "                    'Slope_Spend', 'Slope_Count', 'Slope_Balance']\n",
    "    \n",
    "    cols_to_plot = [c for c in cols_to_plot if c in df.columns]\n",
    "    \n",
    "    if not cols_to_plot:\n",
    "        print(\"âš ï¸ No score columns found to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(cols_to_plot), ncols=2, figsize=(15, 4 * len(cols_to_plot)))\n",
    "    \n",
    "    for i, col in enumerate(cols_to_plot):\n",
    "        sns.histplot(data=df, x=col, hue=target_col, kde=True, element=\"step\", ax=axes[i, 0], palette='Set1')\n",
    "        axes[i, 0].set_title(f'{col} Distribution by Target')\n",
    "        \n",
    "        sns.boxplot(data=df, x=target_col, y=col, ax=axes[i, 1], palette='Set1')\n",
    "        axes[i, 1].set_title(f'{col} Boxplot by Target')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"âœ… Distribution plots displayed.\")\n",
    "\n",
    "def plot_confusion_matrix_heatmap(y_test, y_pred, title):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance_xgb(model, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    xgb.plot_importance(model, max_num_features=20, height=0.5, title=f'Feature Importance - {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_shap_summary(model, X_train, top_n_shap=20):\n",
    "    print(\"\\nğŸ“Š [SHAP Analysis] Calculating SHAP values...\")\n",
    "    \n",
    "    X_shap = X_train\n",
    "    if len(X_train) > 2000:\n",
    "        X_shap = X_train.sample(n=2000, random_state=42)\n",
    "    \n",
    "    # XGBoostëŠ” TreeExplainer ì‚¬ìš© ê°€ëŠ¥\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "    \n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_shap, max_display=top_n_shap)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# 5. Model Training Modules (XGBoost)\n",
    "# ========================================================================================\n",
    "\n",
    "def train_eval_xgboost_main(df_train, target_col='Target'):\n",
    "    print(f\"\\nğŸ‹ï¸ [Model Training] XGBoost (Main Loop)...\")\n",
    "    \n",
    "    leakage_cols = [\n",
    "        target_col, COL_ID, 'Unnamed: 0',\n",
    "        'Slope_Spend', 'Slope_Balance', 'Slope_Count',\n",
    "        'Score_BadDebt', 'Score_Delinq', 'Score_Activity', 'Score_Asset',\n",
    "        'Score_Status_Total', 'Score_Slope_Total', 'Final_Total_Score'\n",
    "    ]\n",
    "    \n",
    "    features = [c for c in df_train.columns if c not in leakage_cols]\n",
    "    numeric_features = df_train[features].select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    X = df_train[numeric_features]\n",
    "    y = df_train[target_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced class\n",
    "    # scale_pos_weight = sum(negative) / sum(positive)\n",
    "    pos_count = y_train.sum()\n",
    "    neg_count = len(y_train) - pos_count\n",
    "    spw = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    print(f\" - Calculated scale_pos_weight: {spw:.4f}\")\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=spw,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\n[XGBoost Results]\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    plot_confusion_matrix_heatmap(y_test, y_pred, \"XGBoost\")\n",
    "    plot_feature_importance_xgb(model, \"XGBoost\")\n",
    "    \n",
    "    # SHAP Visualization\n",
    "    visualize_shap_summary(model, X_train)\n",
    "\n",
    "    return model, X_train\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# 6. Main Execution\n",
    "# ========================================================================================\n",
    "\n",
    "# File Paths\n",
    "DATA_FILE_PATH = './260108/general_combined_part1.csv'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ Starting Feature Selection & Visualization Pipeline (XGBoost Version)...\")\n",
    "    \n",
    "    if os.path.exists(DATA_FILE_PATH):\n",
    "        df_final = process_data_and_merge(DATA_FILE_PATH)\n",
    "        \n",
    "        if df_final is not None:\n",
    "            plot_score_distributions(df_final)\n",
    "            calculate_vif(df_final)\n",
    "            \n",
    "            # Execute XGBoost Training and Evaluation\n",
    "            xgb_model, X_train_xgb = train_eval_xgboost_main(df_final)\n",
    "            \n",
    "            print(\"âœ… Pipeline (XGBoost) Completed Successfully.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Data file not found. Please check DATA_FILE_PATH.\")\n",
    "        print(f\"Current Pwd: {os.getcwd()}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
