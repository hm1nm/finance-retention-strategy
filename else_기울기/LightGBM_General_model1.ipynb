{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LightGBM \n",
        "GENERAL ë°ì´í„° ë¡œë“œ + ëˆ„ì  ê¸°ìš¸ê¸° churn_score ìƒì„± + í´ëŸ¬ìŠ¤í„°ë§ + í´ëŸ¬ìŠ¤í„° ë³„ threshold ê°’ êµ¬í•˜ê¸° + 0ê³¼ 1ë¡œ êµ¬ë¶„ target ìƒì„± + LightGBM ëª¨ë¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No3_vTEWaBUF",
        "outputId": "737c1bc2-a45c-421d-9909-070b8fb8a1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. ë°ì´í„° ë¡œë“œ ê²½ë¡œ ì„¤ì • (ë³¸ì¸ì˜ ë“œë¼ì´ë¸Œ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
        "# ì˜ˆ: '/content/drive/MyDrive/data/vip_customers_all.csv'\n",
        "file_path = '/content/drive/MyDrive/251218/1cha_ì„ í˜„_general_merge_data_240636_64.csv'\n",
        "# output_path = '/content/drive/MyDrive/churn_prediction_result.csv'\n",
        "\n",
        "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "if os.path.exists(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "    # íŒŒì¼ì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì˜ˆì‹œ ì—ëŸ¬ ë°©ì§€ìš© ì¤‘ë‹¨\n",
        "    raise FileNotFoundError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSn4p-XBaW7L",
        "outputId": "037a44bc-8541-4a10-8ff2-90c200e8c3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[201812 ê¸°ì¤€ ì´íƒˆ ìœ„í—˜ ì ìˆ˜ ìƒìœ„ 10ëª…]\n",
            "             ë°œê¸‰íšŒì›ë²ˆí˜¸    ê¸°ì¤€ë…„ì›”  Slope_Spend  Churn_Score\n",
            "203121   SYN_195025  201812    -636369.0         20.2\n",
            "222077  SYN_1611411  201812    -531305.5         19.2\n",
            "211969   SYN_852535  201812    -637794.0         18.9\n",
            "219415  SYN_1406611  201812    -547064.0         16.4\n",
            "231581  SYN_2331501  201812    -472948.5         16.3\n",
            "207160   SYN_498091  201812    -524391.5         16.0\n",
            "207143   SYN_497208  201812    -278261.0         15.8\n",
            "203725   SYN_240953  201812    -573701.0         14.9\n",
            "202994   SYN_186121  201812    -521971.0         14.9\n",
            "230790  SYN_2274803  201812    -452361.5         14.6\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import linregress\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 0. ë°ì´í„° ì •ë ¬ (ì¤‘ìš”: íšŒì›ë³„, ì›”ë³„ ìˆœì„œê°€ ë§ì•„ì•¼ rollingì´ ì •í™•í•©ë‹ˆë‹¤)\n",
        "df = df.sort_values(by=['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”'])\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1. ì´ë™ ê¸°ìš¸ê¸°(Rolling Slope) ê³„ì‚° í•¨ìˆ˜\n",
        "# -------------------------------------------------------\n",
        "def calc_slope(y):\n",
        "    # ë°ì´í„°ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•  ìˆ˜ ì—†ìŒ\n",
        "    if len(y) < 2 or np.sum(y) == 0:\n",
        "        return 0.0\n",
        "    x = np.arange(len(y))\n",
        "    slope, _, _, _, _ = linregress(x, y)\n",
        "    return slope\n",
        "\n",
        "# ìµœê·¼ 3ê°œì›”(window=3)ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸° ì‚°ì¶œ\n",
        "# min_periods=2: ë°ì´í„°ê°€ 2ê°œë§Œ ìŒ“ì—¬ë„ ê¸°ìš¸ê¸° ê³„ì‚° ì‹œì‘\n",
        "group = df.groupby('ë°œê¸‰íšŒì›ë²ˆí˜¸')\n",
        "\n",
        "df['Slope_Spend'] = group['ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M'].rolling(window=3, min_periods=2).apply(calc_slope).reset_index(level=0, drop=True)\n",
        "df['Slope_Balance'] = group['ì”ì•¡_B0M'].rolling(window=3, min_periods=2).apply(calc_slope).reset_index(level=0, drop=True)\n",
        "df['Slope_Count'] = group['ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M'].rolling(window=3, min_periods=2).apply(calc_slope).reset_index(level=0, drop=True)\n",
        "\n",
        "# ê²°ì¸¡ì¹˜(ì²« ë‹¬ ë“±)ëŠ” 0ìœ¼ë¡œ ì±„ì›€\n",
        "df[['Slope_Spend', 'Slope_Balance', 'Slope_Count']] = df[['Slope_Spend', 'Slope_Balance', 'Slope_Count']].fillna(0)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 2. ê¸°ìš¸ê¸°ë¥¼ 'ì´íƒˆ ì ìˆ˜'ë¡œ ë³€í™˜ (ëª¨ë“  í–‰ì— ëŒ€í•´ ê³„ì‚°)\n",
        "# -------------------------------------------------------\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "def convert_slopes_to_scores(dataframe):\n",
        "    # ìŒìˆ˜(ê°ì†Œ)ì¸ ê²½ìš°ë§Œ ì¶”ì¶œí•˜ì—¬ ì–‘ìˆ˜ë¡œ ë³€í™˜ (ê°€ì¥ ë§ì´ ì¤„ì–´ë“  ê²Œ ê°€ì¥ í° ê°’)\n",
        "    # 7ì›”ë¶€í„° 12ì›”ê¹Œì§€ ëª¨ë“  í–‰ì˜ ê¸°ìš¸ê¸°ë¥¼ í•œêº¼ë²ˆì— ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ìƒëŒ€ì  ìœ„ì¹˜ íŒŒì•…\n",
        "    temp_scores = {}\n",
        "    for col in ['Slope_Spend', 'Slope_Balance', 'Slope_Count']:\n",
        "        neg_val = dataframe[col].apply(lambda x: -x if x < 0 else 0)\n",
        "        temp_scores[col] = scaler.fit_transform(neg_val.values.reshape(-1, 1)).flatten()\n",
        "    return temp_scores\n",
        "\n",
        "# ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)\n",
        "norm_scores = convert_slopes_to_scores(df)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 3. ê°€ì¤‘ì¹˜ ì ìš© ë° ìµœì¢… ì ìˆ˜(Churn_Score) ì‚°ì¶œ\n",
        "# -------------------------------------------------------\n",
        "W_SPEND, W_BALANCE, W_COUNT, W_RISK = 40, 30, 20, 10\n",
        "\n",
        "# ë¦¬ìŠ¤í¬ ì ìˆ˜ (ì—°ì²´ë‚˜ ê±°ì ˆì´ ìˆìœ¼ë©´ 1ì , ì—†ìœ¼ë©´ 0ì  -> ë‚˜ì¤‘ì— ê°€ì¤‘ì¹˜ 10 ê³±í•¨)\n",
        "df['Risk_Flag'] = np.where(\n",
        "    (df.get('ì—°ì²´ì”ì•¡_B0M', 0) > 0) | (df.get('ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_B0M', 0) > 0),\n",
        "    1, 0\n",
        ")\n",
        "\n",
        "# ìµœì¢… ìŠ¤ì½”ì–´ í•©ì‚°\n",
        "df['Churn_Score'] = (\n",
        "    (norm_scores['Slope_Spend'] * W_SPEND) +\n",
        "    (norm_scores['Slope_Balance'] * W_BALANCE) +\n",
        "    (norm_scores['Slope_Count'] * W_COUNT) +\n",
        "    (df['Risk_Flag'] * W_RISK)\n",
        ")\n",
        "\n",
        "df['Churn_Score'] = df['Churn_Score'].round(1)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4. ê²°ê³¼ í™•ì¸ (ìµœì‹  ì›”ì¸ 12ì›” ë°ì´í„°ë§Œ ë³´ê¸°)\n",
        "# -------------------------------------------------------\n",
        "target_month = df['ê¸°ì¤€ë…„ì›”'].max()\n",
        "df_final = df[df['ê¸°ì¤€ë…„ì›”'] == target_month].copy()\n",
        "\n",
        "print(f\"\\n[{target_month} ê¸°ì¤€ ì´íƒˆ ìœ„í—˜ ì ìˆ˜ ìƒìœ„ 10ëª…]\")\n",
        "cols_view = ['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”', 'Slope_Spend', 'Churn_Score']\n",
        "print(df_final[cols_view].sort_values('Churn_Score', ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOePPUnDako0"
      },
      "outputs": [],
      "source": [
        "# 1. í™•ì¸í•˜ê³  ì‹¶ì€ ë°œê¸‰íšŒì›ë²ˆí˜¸ ì„¤ì • (ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” ë²ˆí˜¸ë¥¼ ë„£ìœ¼ì„¸ìš”)\n",
        "target_id = df['ë°œê¸‰íšŒì›ë²ˆí˜¸'].iloc[9765]  # ì˜ˆì‹œë¡œ ì²« ë²ˆì§¸ íšŒì›ì˜ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "\n",
        "# 2. í•´ë‹¹ íšŒì›ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ì£¼ìš” ì»¬ëŸ¼ í™•ì¸\n",
        "# ìƒˆë¡œ ë§Œë“  ì»¬ëŸ¼ë“¤ + ê¸°ì¤€ì´ ë˜ëŠ” ì»¬ëŸ¼ë“¤ì„ í•¨ê»˜ ë´…ë‹ˆë‹¤.\n",
        "view_cols = [\n",
        "    'ê¸°ì¤€ë…„ì›”',\n",
        "    'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M', 'Slope_Spend',   # ì‹¤ì œ ì†Œë¹„ëŸ‰ê³¼ ê·¸ ê¸°ìš¸ê¸°\n",
        "    'ì”ì•¡_B0M', 'Slope_Balance',        # ì”ì•¡ê³¼ ê·¸ ê¸°ìš¸ê¸°\n",
        "    'Risk_Flag',                       # ë¦¬ìŠ¤í¬ ì—¬ë¶€\n",
        "    'Churn_Score'                      # ìµœì¢… ì´íƒˆ ìœ„í—˜ ì ìˆ˜\n",
        "]\n",
        "\n",
        "customer_history = df[df['ë°œê¸‰íšŒì›ë²ˆí˜¸'] == target_id][view_cols].sort_values('ê¸°ì¤€ë…„ì›”')\n",
        "\n",
        "print(f\"ğŸ” [íšŒì›ë²ˆí˜¸: {target_id}] ì˜ 6ê°œì›”ê°„ ì´íƒˆ ì§•í›„ ë³€í™”\")\n",
        "print(\"-\" * 100)\n",
        "print(customer_history)\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# 3. (íŒ) ì ìˆ˜ê°€ ê°€ì¥ ë§ì´ ë³€í•œ ë‹¬ ì°¾ê¸°\n",
        "diff = customer_history['Churn_Score'].max() - customer_history['Churn_Score'].min()\n",
        "print(f\"ğŸ’¡ í•´ë‹¹ ê³ ê°ì˜ 6ê°œì›”ê°„ ì ìˆ˜ ë³€ë™í­: {diff:.1f}ì \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9nx4Z-Aarzp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ê°ì ì»´í“¨í„°ì—ì„œ df ë¡œë“œí•˜ê³ \n",
        "# df_final ì— Churn_Score ë§Œë“œëŠ” ì½”ë“œê¹Œì§€ ì‘ì„±í•œ í›„ì— ì•„ë˜ ì½”ë“œ ì‹¤í–‰!\n",
        "\n",
        "\n",
        "# 1. í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  í•µì‹¬ ë³€ìˆ˜ ì„ íƒ\n",
        "# (Churn_Scoreì™€ Slope_Spend ë‘ ê°€ì§€ë¡œ êµ°ì§‘í™”)\n",
        "cluster_cols = ['Churn_Score']\n",
        "\n",
        "# ë°ì´í„° ì¤€ë¹„\n",
        "X_cluster = df[cluster_cols].copy()\n",
        "\n",
        "# 2. ìŠ¤ì¼€ì¼ë§ (í•„ìˆ˜)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_cluster)\n",
        "\n",
        "# 3. K-Means í´ëŸ¬ìŠ¤í„°ë§ (K=3)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# ì›ë³¸ ë°ì´í„°ì— í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë¶™ì´ê¸°\n",
        "df['Cluster_ID'] = clusters\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4. ê²°ê³¼ í•´ì„ (í†µê³„)\n",
        "# -------------------------------------------------------\n",
        "print(\"\\n[í´ëŸ¬ìŠ¤í„°ë³„ í‰ê·  íŠ¹ì„± í™•ì¸]\")\n",
        "print(df.groupby('Cluster_ID')[cluster_cols].mean())\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 5. ì‹œê°í™” (ìˆ˜ì •ë¨: í´ëŸ¬ìŠ¤í„°ë§ ë³€ìˆ˜ì™€ ì¶• ì¼ì¹˜ì‹œí‚¤ê¸°)\n",
        "# -------------------------------------------------------\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Xì¶•: ì†Œë¹„ ê¸°ìš¸ê¸° (Slope_Spend)\n",
        "# Yì¶•: ì´íƒˆ ì ìˆ˜ (Churn_Score)\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x='Slope_Spend',\n",
        "    y='Churn_Score',\n",
        "    hue='Cluster_ID',\n",
        "    palette='viridis',\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "plt.title('Clustering Result: Slope vs Score')\n",
        "plt.xlabel('Slope Spend (ì†Œë¹„ ê¸°ìš¸ê¸°)')\n",
        "plt.ylabel('Churn Score (ì´íƒˆ ì ìˆ˜)')\n",
        "\n",
        "# ê¸°ì¤€ì„  ì¶”ê°€ (ë³´ê¸° í¸í•˜ê²Œ)\n",
        "plt.axvline(x=0, color='r', linestyle='--', label='Slope=0')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 6. í´ëŸ¬ìŠ¤í„° 0 (ë˜ëŠ” íŠ¹ì • ê·¸ë£¹) ë¹„ì¤‘ í™•ì¸ ì½”ë“œ\n",
        "# -------------------------------------------------------\n",
        "# ì „ì²´ ì¸ì›ìˆ˜ ë° ë¹„ìœ¨ ê³„ì‚°\n",
        "cluster_counts = df['Cluster_ID'].value_counts().sort_index()\n",
        "cluster_ratios = df['Cluster_ID'].value_counts(normalize=True).sort_index() * 100\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"[í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬ í˜„í™©]\")\n",
        "df_summary = pd.DataFrame({\n",
        "    'ì¸ì›ìˆ˜(ëª…)': cluster_counts,\n",
        "    'ë¹„ìœ¨(%)': cluster_ratios.round(2)\n",
        "})\n",
        "print(df_summary)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# target_groupì€ ì•„ë˜ì˜ ì‹œê°í™”ë¥¼ ë³´ê³  ë³€ê²½í•´ì¤˜ì•¼í•¨!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
        "# ë§Œì•½ 0ë²ˆ ê·¸ë£¹ì„ ìœ„í—˜êµ°ìœ¼ë¡œ ë³¸ë‹¤ë©´:\n",
        "target_group = 0\n",
        "print(f\"â–¶ [Cluster {target_group}] ìœ„í—˜êµ° ë¹„ìœ¨: {cluster_ratios[target_group]:.2f}%\")\n",
        "print(f\"â–¶ [Cluster {target_group}] ì¸ì›ìˆ˜: {cluster_counts[target_group]:,}ëª…\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lFr2_G9az7l"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "# 1. ê³ ìœ„í—˜êµ°(Cluster 2)ì¸ì§€ ì•„ë‹Œì§€ ë”± ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¼ë²¨ë§ (ì´ì§„ ë¶„ë¥˜ ì¤€ë¹„)\n",
        "# Cluster 2ë©´ 1(High Risk), ì•„ë‹ˆë©´ 0(Others)\n",
        "df['is_high_risk'] = (df['Cluster_ID'] == 2).astype(int)\n",
        "\n",
        "# 2. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ëª¨ë¸ë¡œ 'ê²½ê³„ì„ ' ì°¾ê¸°\n",
        "# (ê¹Šì´ë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ê°€ì¥ í•µì‹¬ì ì¸ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤)\n",
        "tree_model = DecisionTreeClassifier(max_depth=1)\n",
        "tree_model.fit(df[['Churn_Score']], df['is_high_risk'])\n",
        "\n",
        "# 3. ê¸°ì¤€ ì ìˆ˜ ì¶”ì¶œ\n",
        "threshold = tree_model.tree_.threshold[0]\n",
        "\n",
        "print(f\"ğŸ” [ë¶„ì„ ê²°ê³¼] Cluster 2ë¥¼ ë‚˜ëˆ„ëŠ” Churn_Score ê¸°ì¤€ ì ìˆ˜: ì•½ {threshold:.2f}ì \")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 4. í†µê³„ì ìœ¼ë¡œ í™•ì¸ (ìµœì†Œ/ìµœëŒ€ê°’ ë¹„êµ)\n",
        "cluster_stats = df.groupby('Cluster_ID')['Churn_Score'].agg(['min', 'max', 'mean'])\n",
        "print(\"[í´ëŸ¬ìŠ¤í„°ë³„ Churn_Score í†µê³„ ìˆ˜ì¹˜]\")\n",
        "print(cluster_stats)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"ğŸ’¡ ì¦‰, Churn_Scoreê°€ {threshold:.2f}ì ì„ ë„˜ì–´ê°€ë©´ 'Cluster 2(ê³ ìœ„í—˜êµ°)'ì— ì†í•  í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # 1ì°¨ì› ë°ì´í„° ì‹œê°í™” (Strip Plot)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì£¼ì˜: ì´ ì½”ë“œëŠ” ìœ„ìª½ ì…€ì—ì„œ 'Cluster_ID'ê°€ ìƒì„±ëœ í›„ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# yì¶• ì—†ì´ xì¶•(Churn_Score)ë§Œ ì‚¬ìš©í•˜ì—¬ ë¶„í¬ í™•ì¸\n",
        "# jitter=0.3: ì ë“¤ì´ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ìœ„ì•„ë˜ë¡œ í©ë¿Œë¦¼\n",
        "sns.stripplot(\n",
        "    data=df,\n",
        "    x='Churn_Score',\n",
        "    hue='Cluster_ID',   # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ êµ¬ë¶„\n",
        "    palette='viridis',\n",
        "    alpha=0.5,\n",
        "    jitter=0.3\n",
        ")\n",
        "\n",
        "plt.title('1D Scatter Plot (Strip Plot) of Churn Score')\n",
        "plt.xlabel('Churn Score (ì´íƒˆ ìœ„í—˜ ì ìˆ˜)')\n",
        "\n",
        "# ê¸°ì¤€ì„  í‘œì‹œ (ì´ì „ ë¶„ì„ì˜ ì„ê³„ê°’ ì°¸ê³ )\n",
        "plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.2f})')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDQOBcZJa2S6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ë°©ë²• 1: np.where í™œìš© (ê°€ì¥ ì§ê´€ì ì´ê³  ë¹ ë¦„)\n",
        "# ì¡°ê±´, ì°¸ì¼ ë•Œ ê°’, ê±°ì§“ì¼ ë•Œ ê°’ ìˆœì„œì…ë‹ˆë‹¤.\n",
        "df['churn_target'] = np.where(df['Churn_Score'] >= threshold, 1, 0)\n",
        "\n",
        "# ë°©ë²• 2: ë¶ˆë¦¬ì–¸ ë³€í™˜ í™œìš© (ì½”ë“œê°€ ê°„ê²°í•¨)\n",
        "# df['churn_target'] = (df['Churn_Score'] >= 10.35).astype(int)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "# -------------------------------------------------------\n",
        "print(\"[íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ í™•ì¸]\")\n",
        "print(df['churn_target'].value_counts())\n",
        "\n",
        "print(\"\\n[ìƒìœ„ 5ê°œ ë°ì´í„° í™•ì¸]\")\n",
        "print(df[['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'Churn_Score', 'churn_target']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps0SZaN0a65z"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIe1Ak3Fa_-U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# í•µì‹¬ 1: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì •ì˜\n",
        "# ============================================================================\n",
        "target_col = 'churn_target'  # ì •ë‹µì§€\n",
        "drop_cols = ['churn_target', 'Churn_Score','Slope_Spend','Slope_Balance','Slope_Count', 'ë°œê¸‰íšŒì›ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”',\n",
        "    'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M',  'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M','ì”ì•¡_B0M','Risk_Flag','Cluster_ID','is_high_risk']\n",
        "\n",
        "# ============================================================================\n",
        "# í•µì‹¬ 2: ì›”ë³„ë¡œ ë°ì´í„° ìª¼ê°œê¸° (Time Split)\n",
        "# ============================================================================\n",
        "# 1) í•™ìŠµ ë°ì´í„° (7, 8, 9, 10ì›”)\n",
        "train_df = df[df['ê¸°ì¤€ë…„ì›”'].isin([201807, 201808, 201809, 201810])]\n",
        "\n",
        "# 2) ê²€ì¦ ë°ì´í„° (11ì›”) - ëª¨ë¸ ì±„ì ìš©\n",
        "valid_df = df[df['ê¸°ì¤€ë…„ì›”'] == 201811]\n",
        "\n",
        "# 3) ì‹¤ì „ ì˜ˆì¸¡ ë°ì´í„° (12ì›”) - ì •ë‹µ(Target)ì´ ì—†ëŠ” ë¯¸ë˜ ë°ì´í„°\n",
        "predict_df = df[df['ê¸°ì¤€ë…„ì›”'] == 201812]\n",
        "\n",
        "# ============================================================================\n",
        "# í•µì‹¬ 3: X(ë¬¸ì œ), y(ì •ë‹µ) ë‚˜ëˆ„ê¸°\n",
        "# ============================================================================\n",
        "X_train = train_df.drop(columns=drop_cols)\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "X_valid = valid_df.drop(columns=drop_cols)\n",
        "y_valid = valid_df[target_col]\n",
        "\n",
        "X_predict = predict_df.drop(columns=drop_cols)\n",
        "predict_ids = predict_df['ë°œê¸‰íšŒì›ë²ˆí˜¸'] # ê²°ê³¼ ë§¤ì¹­ìš© ID í‚µ\n",
        "\n",
        "# ë¬¸ìì—´ -> category ë³€í™˜ (LightGBM ìµœì í™”)\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype == 'object':\n",
        "        X_train[col] = X_train[col].astype('category')\n",
        "        X_valid[col] = X_valid[col].astype('category')\n",
        "        X_predict[col] = X_predict[col].astype('category')\n",
        "\n",
        "# ============================================================================\n",
        "# í•µì‹¬ 4: ëª¨ë¸ í•™ìŠµ\n",
        "# ============================================================================\n",
        "model = lgb.LGBMClassifier(n_estimators=1000, random_state=42, verbose=-1)\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_valid, y_valid)],\n",
        "    eval_metric='logloss',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# í•µì‹¬ 5: 12ì›” ê³ ê°ì— ëŒ€í•œ ì´íƒˆ í™•ë¥  ì˜ˆì¸¡ ë° ê²°ê³¼ ì €ì¥\n",
        "# ============================================================================\n",
        "pred_prob = model.predict_proba(X_predict)[:, 1]\n",
        "\n",
        "# ê²°ê³¼ ì •ë¦¬\n",
        "final_result = pd.DataFrame({\n",
        "    'ë°œê¸‰íšŒì›ë²ˆí˜¸': predict_ids,\n",
        "    'ì´íƒˆì˜ˆì¸¡í™•ë¥ ': pred_prob\n",
        "})\n",
        "\n",
        "# ê²°ê³¼ ì •ë¦¬\n",
        "final_result = pd.DataFrame({\n",
        "    'ë°œê¸‰íšŒì›ë²ˆí˜¸': predict_ids,\n",
        "    'ì´íƒˆì˜ˆì¸¡í™•ë¥ ': pred_prob\n",
        "})\n",
        "\n",
        "print(final_result.sort_values('ì´íƒˆì˜ˆì¸¡í™•ë¥ ', ascending=False).head(10))\n",
        "\n",
        "# # ë“œë¼ì´ë¸Œì— ê²°ê³¼ ì €ì¥ (CSV)\n",
        "# final_result.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# print(f\"ì˜ˆì¸¡ ê²°ê³¼ê°€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n",
        "# print(final_result.sort_values('ì´íƒˆì˜ˆì¸¡í™•ë¥ ', ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inz9pGZdbCWB"
      },
      "outputs": [],
      "source": [
        "pred_valid = model.predict(X_valid)\n",
        "print(\"ê²€ì¦(11ì›”) ì •í™•ë„:\", accuracy_score(y_valid, pred_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’\n",
        "y_pred = model.predict(X_valid)\n",
        "\n",
        "# ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
        "print(classification_report(y_valid, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
