import os
import argparse
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support

def safe_read_csv(path):
    """
    CSV íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(path, low_memory=False)
        print(f"[ë¡œë“œ ì„±ê³µ] shape = {df.shape}")
        return df
    except Exception as e:
        print(f"âŒ [ë¡œë“œ ì‹¤íŒ¨] {e}")
        return None

def generate_target_method_b(df, ratio_threshold=0.8):
    """
    VIP íƒ€ê²Ÿ ìƒì„± (Method B: 20% ê°ì†Œ ë£°)
    - Target = 1 (ì´íƒˆ): ë‹¹ì›” ì´ ì´ìš©ê¸ˆì•¡ < (ì§ì „ 3ê°œì›” í‰ê·  * 0.8)
    - Target = 0 (ìœ ì§€): ê·¸ ì™¸
    - íŒë‹¨ ì œì™¸(NaN): ì§ì „ 3ê°œì›” í‰ê· ì´ 0/ê²°ì¸¡ì¸ ê²½ìš°(íœ´ë©´/ì‹ ê·œ)
    """
    if df is None:
        return None, None

    required_cols = [
        "ë°œê¸‰íšŒì›ë²ˆí˜¸", "ê¸°ì¤€ë…„ì›”",
        "ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_B0M",
        "ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R3M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_R3M",
    ]
    missing = [c for c in required_cols if c not in df.columns]
    if len(missing) > 0:
        print("âŒ [íƒ€ê²Ÿ ìƒì„± ì‹¤íŒ¨] í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½:", missing)
        return None, None

    work = df.copy()

    # ì •ë ¬(íšŒì›/ì›” ê¸°ì¤€) â€” ì›ë³¸ ì˜ë„ ìœ ì§€
    try:
        work = work.sort_values(by=["ë°œê¸‰íšŒì›ë²ˆí˜¸", "ê¸°ì¤€ë…„ì›”"]).copy()
    except Exception as e:
        print("âš ï¸ [ì •ë ¬ ê²½ê³ ] ì •ë ¬ ì‹¤íŒ¨:", e)

    # ê²°ì¸¡ 0 ì²˜ë¦¬(íƒ€ê²Ÿ ìƒì„±ìš© ì»¬ëŸ¼ë§Œ)
    for c in ["ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_B0M", "ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R3M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_R3M"]:
        work[c] = work[c].fillna(0)

    # ë‹¹ì›”/ì§ì „3M í‰ê·  ì‚°ì¶œ
    work["ë‹¹ì›”_ì´_ì´ìš©ê¸ˆì•¡"] = work["ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M"] + work["ì´ìš©ê¸ˆì•¡_ì²´í¬_B0M"]
    work["ì§ì „_3M_í‰ê· _ì´ìš©ê¸ˆì•¡"] = (work["ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R3M"] + work["ì´ìš©ê¸ˆì•¡_ì²´í¬_R3M"]) / 3

    # Target ì´ˆê¸°í™”
    work["Target"] = np.nan

    # íŒë‹¨ ê°€ëŠ¥ ì¡°ê±´: ì§ì „ 3M í‰ê·  > 0
    can_judge = work["ì§ì „_3M_í‰ê· _ì´ìš©ê¸ˆì•¡"] > 0

    # ë¹„ìœ¨ ê³„ì‚° ë° íƒ€ê²Ÿ ë¶€ì—¬
    ratio = pd.Series(np.nan, index=work.index)
    ratio.loc[can_judge] = work.loc[can_judge, "ë‹¹ì›”_ì´_ì´ìš©ê¸ˆì•¡"] / work.loc[can_judge, "ì§ì „_3M_í‰ê· _ì´ìš©ê¸ˆì•¡"]

    work.loc[can_judge & (ratio < ratio_threshold), "Target"] = 1
    work.loc[can_judge & (ratio >= ratio_threshold), "Target"] = 0

    train_df = work[work["Target"].notna()].copy()
    train_df["Target"] = train_df["Target"].astype(int)

    dormant_df = work[work["Target"].isna()].copy()

    print("\n" + "="*50)
    print("ğŸ“Š íƒ€ê²Ÿ ìƒì„± ê²°ê³¼ (Method B: 20% ê°ì†Œ ë£°)")
    print("="*50)
    print(f" - ì „ì²´ ë°ì´í„°: {len(work)}ê±´")
    print(f" - í•™ìŠµìš© ë°ì´í„°(0/1): {len(train_df)}ê±´")
    if len(train_df) > 0:
        print(f"   â”” ì´íƒˆ(1): {(train_df['Target']==1).sum()}ê±´ ({train_df['Target'].mean()*100:.2f}%)")
        print(f"   â”” ìœ ì§€(0): {(train_df['Target']==0).sum()}ê±´")
    print(f" - íŒë‹¨ ì œì™¸(NaN): {len(dormant_df)}ê±´")
    print("="*50)

    return train_df, dormant_df

def prepare_Xy_group_split(train_df, id_col="ë°œê¸‰íšŒì›ë²ˆí˜¸", target_col="Target", drop_b0m=True):
    """
    ëª¨ë¸ ì…ë ¥ ë°ì´í„° êµ¬ì„± (ëˆ„ìˆ˜ ì œê±° + ë°œê¸‰íšŒì›ë²ˆí˜¸ ê¸°ì¤€ ë¶„ë¦¬)
    """
    if train_df is None or len(train_df) == 0:
        print("âŒ [ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨] í•™ìŠµìš© train_dfê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return None, None, None, None

    if id_col not in train_df.columns or target_col not in train_df.columns:
        print("âŒ [ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨] í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½:", id_col, target_col)
        return None, None, None, None

    data = train_df.copy()

    # íƒ€ê²Ÿ ìƒì„±ì— ì‚¬ìš©ëœ ì»¬ëŸ¼(ëˆ„ìˆ˜ ê°€ëŠ¥) ì œê±°
    leakage_cols = [
        "ë‹¹ì›”_ì´_ì´ìš©ê¸ˆì•¡", "ì§ì „_3M_í‰ê· _ì´ìš©ê¸ˆì•¡",
        "ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_B0M",
        "ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R3M", "ì´ìš©ê¸ˆì•¡_ì²´í¬_R3M",
    ]

    # B0M í¬í•¨ ì»¬ëŸ¼ì€ ë‹¹ì›” ì •ë³´ì´ë¯€ë¡œ í†µì§¸ë¡œ ì œê±°(ì›ë³¸ ì˜ë„)
    b0m_cols = []
    if drop_b0m:
        b0m_cols = [c for c in data.columns if "B0M" in c]

    drop_cols = list(set([id_col, "ê¸°ì¤€ë…„ì›”", target_col] + leakage_cols + b0m_cols))
    drop_cols_exist = [c for c in drop_cols if c in data.columns]

    X = data.drop(columns=drop_cols_exist, errors="ignore").copy()
    y = data[target_col].astype(int).copy()

    # íšŒì› ë‹¨ìœ„ë¡œ stratify (íšŒì›ë³„ target 1ê°œë§Œ ì¶”ì¶œ)
    unique_ids = data[[id_col, target_col]].drop_duplicates(subset=[id_col]).copy()
    try:
        train_ids, test_ids = train_test_split(
            unique_ids[id_col],
            test_size=0.2,
            random_state=42,
            stratify=unique_ids[target_col]
        )
    except Exception as e:
        # stratify ë¶ˆê°€ëŠ¥(ì†Œìˆ˜ í´ë˜ìŠ¤ê°€ ë„ˆë¬´ ì ê±°ë‚˜ ë“±) ì‹œ fallback
        print("âš ï¸ stratify split ì‹¤íŒ¨ â†’ ë¹„ì¸µí™” splitë¡œ ëŒ€ì²´:", e)
        train_ids, test_ids = train_test_split(
            unique_ids[id_col],
            test_size=0.2,
            random_state=42
        )

    train_mask = data[id_col].isin(train_ids)
    test_mask = data[id_col].isin(test_ids)

    X_train = X.loc[train_mask].copy()
    y_train = y.loc[train_mask].copy()
    X_test = X.loc[test_mask].copy()
    y_test = y.loc[test_mask].copy()

    # ê·¸ë£¹ ëˆ„ìˆ˜ ì²´í¬
    overlap = set(data.loc[train_mask, id_col]).intersection(set(data.loc[test_mask, id_col]))
    print("[ê·¸ë£¹ ëˆ„ìˆ˜ ì²´í¬] ê²¹ì¹˜ëŠ” íšŒì› ìˆ˜ =", len(overlap))

    print("\n[ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ]")
    print(" - X_train:", X_train.shape, " y_train:", y_train.shape, " (ì´íƒˆë¹„ì¤‘:", round(y_train.mean(), 4), ")")
    print(" - X_test :", X_test.shape,  " y_test :", y_test.shape,  " (ì´íƒˆë¹„ì¤‘:", round(y_test.mean(), 4), ")")

    return X_train, X_test, y_train, y_test

def encode_and_fillna(X_train, X_test):
    """
    ë²”ì£¼í˜• ì²˜ë¦¬(LabelEncoder) + ê²°ì¸¡ ì²˜ë¦¬
    """
    if X_train is None:
        return None, None

    Xtr = X_train.copy()
    Xte = X_test.copy()

    cat_cols = Xtr.select_dtypes(include=["object"]).columns.tolist()

    for col in cat_cols:
        try:
            le = LabelEncoder()
            full = pd.concat([Xtr[col], Xte[col]], axis=0).astype(str)
            le.fit(full)
            Xtr[col] = le.transform(Xtr[col].astype(str))
            Xte[col] = le.transform(Xte[col].astype(str))
        except Exception as e:
            # í•´ë‹¹ ì»¬ëŸ¼ì—ì„œ ì¸ì½”ë”©ì´ ë¬¸ì œê°€ ë‚˜ë©´ ê³¼ê°íˆ ì œê±°(ì—ëŸ¬ ë°©ì§€)
            print(f"âš ï¸ [ì¸ì½”ë”© ì‹¤íŒ¨] ì»¬ëŸ¼ ì œê±°: {col} / ì‚¬ìœ : {e}")
            Xtr = Xtr.drop(columns=[col], errors="ignore")
            Xte = Xte.drop(columns=[col], errors="ignore")

    # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
    Xtr = Xtr.fillna(-999)
    Xte = Xte.fillna(-999)

    print("[ì¸ì½”ë”©/ê²°ì¸¡ ì²˜ë¦¬ ì™„ë£Œ] X_train:", Xtr.shape, "X_test:", Xte.shape)
    return Xtr, Xte

def run_random_forest(X_train, X_test, y_train, y_test):
    """
    RandomForest í•™ìŠµ + í‰ê°€
    """
    if X_train is None or X_test is None:
        print("âŒ [RF ì‹¤í–‰ ë¶ˆê°€] ì…ë ¥ ë°ì´í„°ê°€ None ì…ë‹ˆë‹¤.")
        return None

    if len(np.unique(y_train)) < 2:
        print("âŒ [RF ì‹¤í–‰ ë¶ˆê°€] y_trainì— í´ë˜ìŠ¤ê°€ 1ê°œë¿ì…ë‹ˆë‹¤. (ì´íƒˆ/ìœ ì§€ ë‘˜ ë‹¤ ìˆì–´ì•¼ í•¨)")
        return None

    model = RandomForestClassifier(
        n_estimators=500,
        max_depth=None,
        max_features="sqrt",
        min_samples_split=2,
        min_samples_leaf=1,
        n_jobs=-1,
        random_state=42,
        class_weight="balanced_subsample"
    )

    print("\n[RandomForest í•™ìŠµ ì‹œì‘...]")
    model.fit(X_train, y_train)

    print("[í•™ìŠµ ì™„ë£Œ, ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...]")
    proba = model.predict_proba(X_test)[:, 1]
    pred = (proba >= 0.5).astype(int)

    cm = confusion_matrix(y_test, pred)
    print("\n[RandomForest Confusion Matrix]\n", cm)

    print("\n[Classification Report]")
    print(classification_report(y_test, pred, digits=4))

    try:
        auc = roc_auc_score(y_test, proba)
        print("[ROC-AUC] =", round(auc, 4))
    except Exception as e:
        auc = None
        print("âš ï¸ ROC-AUC ê³„ì‚° ì‹¤íŒ¨:", e)

    prec, rec, f1, _ = precision_recall_fscore_support(y_test, pred, average="binary", zero_division=0)
    print(f"[ìš”ì•½] Precision={prec:.4f} | Recall={rec:.4f} | F1={f1:.4f} | AUC={auc}")

    result = {
        "model": model,
        "y_proba": proba,
        "y_pred": pred,
        "confusion_matrix": cm,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "auc": auc,
    }
    return result

def optimize_threshold(res_rf, y_test):
    """
    Threshold íŠœë‹ (Recall ì¤‘ì‹¬)
    """
    if res_rf is None:
        print("âš ï¸ RF ê²°ê³¼ê°€ ì—†ì–´ì„œ Threshold íŠœë‹ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return

    proba = res_rf["y_proba"]
    thresholds = np.arange(0.05, 0.96, 0.05)

    rows = []
    for t in thresholds:
        pred = (proba >= t).astype(int)
        prec, rec, f1, _ = precision_recall_fscore_support(y_test, pred, average="binary", zero_division=0)
        rows.append([t, prec, rec, f1])

    df_thr = pd.DataFrame(rows, columns=["threshold", "precision", "recall", "f1"])
    print("\n[Threshold íŠœë‹ ê²°ê³¼ (Top 15)]")
    print(df_thr.sort_values(by=["recall", "precision"], ascending=False).head(15))

def main():
    parser = argparse.ArgumentParser(description="VIP Random Forest Model Pipeline")
    parser.add_argument("--data_path", type=str, default="../../DATA/VIP/VIP_combined_part0.csv", help="Input CSV file path")
    parser.add_argument("--ratio_threshold", type=float, default=0.8, help="Ratio threshold for target generation")
    
    args = parser.parse_args()

    print(f"[ì‹œì‘] ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = safe_read_csv(args.data_path)
    if df is None:
        return

    # 2. íƒ€ê²Ÿ ìƒì„±
    train_data, dormant_data = generate_target_method_b(df, ratio_threshold=args.ratio_threshold)
    
    # 3. ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = prepare_Xy_group_split(train_data)

    # 4. ì „ì²˜ë¦¬ (ì¸ì½”ë”©/ê²°ì¸¡)
    X_train_enc, X_test_enc = encode_and_fillna(X_train, X_test)

    # 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    res_rf = run_random_forest(X_train_enc, X_test_enc, y_train, y_test)

    # 6. Threshold íŠœë‹
    if res_rf is not None:
        optimize_threshold(res_rf, y_test)

if __name__ == "__main__":
    main()
