
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
import xgboost as xgb
from scipy.stats import linregress
import os
import re
import warnings
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve

# =============================================================================
# [ì„¤ì •] í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# =============================================================================
plt.rc('font', family='Malgun Gothic')
plt.rc('axes', unicode_minus=False)
warnings.filterwarnings('ignore')

COL_ID = 'ë°œê¸‰íšŒì›ë²ˆí˜¸'
COL_DATE = 'ê¸°ì¤€ë…„ì›”'

# ë¶„ì„ ë³€ìˆ˜ (Wide Formatì˜ ì ‘ë¯¸ì‚¬ '_MM' ë“±ì„ ë—€ ìˆœìˆ˜ ì»¬ëŸ¼ëª… ê°€ì •)
COL_SPEND = 'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M'      # ì†Œë¹„
COL_COUNT = 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M'      # ë¹ˆë„
COL_COUNT_SPEND = 'ì´ìš©ê±´ìˆ˜_ì‹ íŒ_B0M' # ë¹ˆë„ ì†Œë¹„ (í• ë¶€, ì¼ì‹œë¶ˆ)
COL_BALANCE = 'ì”ì•¡_B0M'             # ì”ì•¡
COL_CASH_ADV = 'ì”ì•¡_í˜„ê¸ˆì„œë¹„ìŠ¤_B0M' # ì•…ì„±ë¶€ì±„1
COL_CARD_LOAN = 'ì”ì•¡_ì¹´ë“œë¡ _B0M'    # ì•…ì„±ë¶€ì±„2
COL_DELINQ = 'ì—°ì²´ì”ì•¡_B0M'          # ë¦¬ìŠ¤í¬3
COL_AVG_BAL = 'ì›”ì¤‘í‰ì”'             # ìì‚°

# Additional columns for R12M fallback
COL_SPEND_R12M = 'ì´ìš©ê¸ˆì•¡_ì‹ ìš©_R12M'
COL_COUNT_R12M = 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_R12M'

# ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
LEAKAGE_COLS = [
    'Target', 'ë°œê¸‰íšŒì›ë²ˆí˜¸', 'Unnamed: 0', 'ê¸°ì¤€ë…„ì›”',
    'Slope_Spend', 'Slope_Balance', 'Slope_Count', 
    'Norm_Slope_Spend', 'Norm_Slope_Balance', 'Norm_Slope_Count',
    'Score_BadDebt', 'Score_Delinq', 'Score_Activity', 'Score_Asset',
    'Score_Status_Total', 'Score_Slope_Total', 'Final_Total_Score',
    'Risk_Count', 'Churn_Segment',
    'Cond1_Has_BadDebt', 'Cond2_Has_Delinq', 'Cond3_Activity_Drop', 'Cond4_Asset_Zero',
    'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_R6M', 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_R3M', 'ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R6M', 'ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R3M', 'ì´ìš©ê±´ìˆ˜_ì‹ íŒ_R6M', 'ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M',
    'ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_R3M','ìµœì¢…ì´ìš©ì¼ì_ê¸°ë³¸', 'ì´ìš©ê±´ìˆ˜_ì‹ íŒ_B0M','ìµœì¢…ì´ìš©ì¼ì_ì‹ íŒ', 'ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_B0M',
    'ìµœì¢…ì´ìš©ì¼ì_ì¼ì‹œë¶ˆ', 'ì´ìš©í›„ê²½ê³¼ì›”_ì¼ì‹œë¶ˆ', '_1ìˆœìœ„ì¹´ë“œì´ìš©ê±´ìˆ˜'
]

# =============================================================================
# [Helper Functions] ê³„ì‚° ë° ë¡œì§
# =============================================================================
def calc_slope_long(series):
    """ì‹œê³„ì—´ ë°ì´í„°(Series)ì˜ ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°"""
    y = series.values.astype(float)
    if len(y) < 2 or np.sum(y) == 0:
        return 0
    x = np.arange(len(y))
    slope, _, _, _, _ = linregress(x, y)
    return 0 if np.isnan(slope) else slope

def calculate_churn_scores(group):
    """ê³ ê° í•œ ëª…ì˜ ë°ì´í„°ë¥¼ ë°›ì•„ ì ìˆ˜ ë° Target ìƒì„± (1ê°œì›” ì´ìƒ ë°ì´í„° í•„ìš”)"""
    # ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
    if len(group) < 1:
        return pd.Series({
            'Score_BadDebt': 0, 'Score_Delinq': 0, 'Score_Activity': 0, 'Score_Asset': 0,
            'Score_Status_Total': 0, 'Slope_Spend': 0, 'Slope_Balance': 0, 'Slope_Count': 0
        })

    # (A) ìƒíƒœ ì ìˆ˜ (Status Score) ì„¸ë¶€ í•­ëª© ê³„ì‚°
    try:
        # Helper for safe indexing
        def get_val(col, idx_from_last):
            if len(group) >= idx_from_last:
                return group[col].iloc[-idx_from_last]
            return 0

        # 1. [ë¶€ì •] ì•…ì„± ë¶€ì±„ ì ìˆ˜ (Score_BadDebt)
        val_last = get_val(COL_CASH_ADV, 1)
        val_prev = get_val(COL_CASH_ADV, 2)
        
        loan_last = get_val(COL_CARD_LOAN, 1)
        loan_prev = get_val(COL_CARD_LOAN, 2)
        
        bad_debt_score = (
            ((val_last - val_prev) / (val_prev + 1) * 1.5) +
            ((loan_last - loan_prev) / (loan_prev + 1) * 1.0)
        )
        
        # 2. [ë¶€ì •] ì—°ì²´ ê°•ë„ ì ìˆ˜ (Score_Delinq)
        delinq_score = (get_val(COL_DELINQ, 1) * 3.0) + (get_val(COL_DELINQ, 2) * 2.0)
        if len(group) >= 3:
            delinq_score += (get_val(COL_DELINQ, 3) * 1.0)
        
        # 3. [ê¸ì •] í™œë™ì„± ì ìˆ˜ (Score_Activity)
        sum_r3 = group[COL_COUNT_SPEND].iloc[-3:].sum()
        sum_r6 = group[COL_COUNT_SPEND].sum()
        activity_score = ((sum_r3 * 2) - sum_r6) / (sum_r6 + 1) * 100
        
        # 4. [ê¸ì •] ìì‚° ë°©ì–´ ì ìˆ˜ (Score_Asset)
        avg_r3 = group[COL_AVG_BAL].iloc[-3:].mean()
        avg_r6 = group[COL_AVG_BAL].mean()
        asset_score = (avg_r3 / (avg_r6 + 1)) * 10
        
        # >> [Total] ìƒíƒœ ì¢…í•© ì ìˆ˜ (Score_Status_Total)
        score_status_total = (bad_debt_score + delinq_score) - (activity_score + asset_score)
    except:
        bad_debt_score = 0
        delinq_score = 0
        activity_score = 0
        asset_score = 0
        score_status_total = 0

    # (B) ê¸°ìš¸ê¸° ì ìˆ˜ (Slope Score)
    # CASE 1: Data >= 2 months (Use linregress)
    if len(group) >= 2:
        slope_spend = calc_slope_long(group[COL_SPEND])
        slope_balance = calc_slope_long(group[COL_BALANCE])
        slope_count = calc_slope_long(group[COL_COUNT])
    
    # CASE 2: Data == 1 month (Use R12M fallback)
    else:
        # Spending Slope Proxy: Current - Monthly_Avg(R12M)
        r12m_spend = group[COL_SPEND_R12M].iloc[0] if COL_SPEND_R12M in group.columns else 0
        avg_spend = r12m_spend / 12
        slope_spend = group[COL_SPEND].iloc[0] - avg_spend
        
        # Count Slope Proxy
        r12m_count = group[COL_COUNT_R12M].iloc[0] if COL_COUNT_R12M in group.columns else 0
        avg_count = r12m_count / 12
        slope_count = group[COL_COUNT].iloc[0] - avg_count
        
        # Balance Slope: Set to -1 (safe condition) as requested
        slope_balance = -1

    return pd.Series({
        'Score_BadDebt': bad_debt_score,
        'Score_Delinq': delinq_score,
        'Score_Activity': activity_score,
        'Score_Asset': asset_score,
        'Score_Status_Total': score_status_total,
        'Slope_Spend': slope_spend,
        'Slope_Balance': slope_balance,
        'Slope_Count': slope_count
    })

def check_churn_condition(scores):
    """Calculates Target (1 or 0) from scores series"""
    # (ì¡°ê±´ A) ê¸°ìš¸ê¸° 3ì¢…(ì†Œë¹„, ì”ì•¡, ê±´ìˆ˜)ì´ ëª¨ë‘ 0 ì´í•˜
    cond_slopes_decrease = (
        (scores['Slope_Spend'] <= 0) & 
        (scores['Slope_Balance'] <= 0) & 
        (scores['Slope_Count'] <= 0)
    )
    
    # (ì¡°ê±´ B) 4ëŒ€ ìœ„í—˜ ì§•í›„ ì¤‘ 1ê°œ ì´ìƒ ê°ì§€ (Risk_Count >= 1 ë¡œ ìˆ˜ì •ë¨)
    cond1 = scores['Score_BadDebt'] > 0
    cond2 = scores['Score_Delinq'] > 0
    cond3 = scores['Score_Activity'] < 0
    cond4 = scores['Score_Asset'] == 0
    
    risk_count = int(cond1) + int(cond2) + int(cond3) + int(cond4)
    cond_high_risk = (risk_count >= 1)
    
    return 1 if (cond_slopes_decrease and cond_high_risk) else 0

# =============================================================================
# [Analysis Functions] ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
# =============================================================================
def analyze_rolling_churn(file_path):
    """ì´íƒˆì Rolling ë¶„ì„: ì´íƒˆ ì§€ì† ê¸°ê°„ í™•ì¸"""
    print(f"\n[Info] íŒŒì¼ ë¡œë“œ ë° ë¶„ì„ ì‹œì‘: {file_path}")
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None, None

    try:
        df = pd.read_csv(file_path, low_memory=False)
        required_cols = [
            COL_ID, COL_DATE, COL_SPEND, COL_COUNT, COL_BALANCE, 
            COL_CASH_ADV, COL_CARD_LOAN, COL_DELINQ, COL_AVG_BAL,
            COL_SPEND_R12M, COL_COUNT_R12M, COL_COUNT_SPEND
        ]
        # ì—†ëŠ” ì»¬ëŸ¼ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        for c in required_cols:
            if c not in df.columns: df[c] = 0
            
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

    # Sort
    df.sort_values(by=[COL_ID, COL_DATE], inplace=True)
    grouped = df.groupby(COL_ID)
    
    results = []
    print(" - ê³ ê°ë³„ Rolling Analysis ì§„í–‰ ì¤‘...")
    
    count_churners = 0
    
    for cust_id, group in grouped:
        if len(group) < 1: continue
            
        current_scores = calculate_churn_scores(group)
        is_current_churn = check_churn_condition(current_scores)
        
        if is_current_churn == 1:
            count_churners += 1
            consecutive_months = 1 
            max_lookback = len(group) - 1
            
            for i in range(1, max_lookback + 1):
                past_group = group.iloc[:-i] 
                past_scores = calculate_churn_scores(past_group)
                is_past_churn = check_churn_condition(past_scores)
                
                if is_past_churn == 1:
                    consecutive_months += 1
                else:
                    break 
            
            results.append({
                COL_ID: cust_id,
                'Churn_Duration_Months': consecutive_months
            })

    if not results:
        print("âŒ ë¶„ì„ëœ ì´íƒˆìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, df

    df_res = pd.DataFrame(results)
    
    print("\n" + "="*50)
    print(f"ğŸ“Š ì´íƒˆì Rolling ë¶„ì„ ê²°ê³¼")
    print("="*50)
    print(f" - ì´ ë¶„ì„ ê³ ê° ìˆ˜: {len(grouped)}ëª…")
    print(f" - ìµœì¢… ì‹œì  ì´íƒˆì ìˆ˜: {len(df_res)}ëª… ({len(df_res)/len(grouped)*100:.2f}%)")
    print("-" * 30)
    print(" [ì´íƒˆ ì§•í›„ ì§€ì† ê¸°ê°„ í†µê³„]")
    print(df_res['Churn_Duration_Months'].describe())
    
    return df_res, df

def analyze_and_extract_features_v2(input_data):
    """íŠ¹ì„± ì¶”ì¶œ ë° MLìš© ë°ì´í„° ì¤€ë¹„ (ì ìˆ˜ ë°ì´í„° ìƒì„±)"""
    if isinstance(input_data, pd.DataFrame):
        print(f"\n[Info] DataFrame ì…ë ¥ë¨ - ë¶„ì„ ì‹œì‘")
        df = input_data.copy()
    elif isinstance(input_data, str) and os.path.exists(input_data):
        print(f"\n[Info] íŒŒì¼ ë¡œë“œ ë° ë¶„ì„ ì‹œì‘: {input_data}")
        df = pd.read_csv(input_data, low_memory=False)
    else:
        print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        return None, None

    # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
    required_cols = [
        COL_ID, COL_DATE, COL_SPEND, COL_COUNT, COL_BALANCE, 
        COL_CASH_ADV, COL_CARD_LOAN, COL_DELINQ, COL_AVG_BAL, 
        COL_COUNT_SPEND
    ]
    for c in required_cols:
        if c not in df.columns: df[c] = 0

    df.sort_values(by=[COL_ID, COL_DATE], inplace=True)
    grouped = df.groupby(COL_ID)
    
    results = []
    print(" - ê³ ê°ë³„ Feature Extraction ì§„í–‰ ì¤‘ (Scores ê³„ì‚°)...")
    
    for cust_id, group in grouped:
        if len(group) < 1: continue
        
        scores = calculate_churn_scores(group)
        is_churn = check_churn_condition(scores)
        
        row_data = scores.to_dict()
        row_data[COL_ID] = cust_id
        row_data['Target'] = is_churn
        results.append(row_data)
        
    df_res = pd.DataFrame(results)
    print(f"âœ… Feature Extraction ì™„ë£Œ: {len(df_res)}ê±´")
    return df_res, df

def make_ml_dataset_final(df_raw, df_scores_viz=None):
    """MLìš© ìµœì¢… ë°ì´í„°ì…‹ ë³‘í•© (Features + Target)"""
    print(f"[Info] ëª¨ë¸ í•™ìŠµìš© ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")

    # 1. Target ë°ì´í„° ì¤€ë¹„
    if df_scores_viz is not None:
        print(" - ê¸°ì¡´ Score ë°ì´í„° í™œìš©")
        df_target = df_scores_viz.copy()
    else:
        print(" - Score ë°ì´í„° ìƒˆë¡œ ê³„ì‚°")
        df_target = df_raw.groupby(COL_ID).apply(calculate_churn_scores).reset_index()

    # Target ë¼ë²¨ë§ (ì¤‘ë³µ ê³„ì‚° ë°©ì§€ ìœ„í•´ ë¡œì§ ì¬ì ìš©)
    cond_slopes_decrease = (
        (df_target['Slope_Spend'] <= 0) & 
        (df_target['Slope_Balance'] <= 0) & 
        (df_target['Slope_Count'] <= 0)
    )
    
    score_cols = ['Score_BadDebt', 'Score_Delinq', 'Score_Activity', 'Score_Asset']
    risk_count = 0
    for col in score_cols:
        if col in df_target.columns:
            if col == 'Score_Activity':
                risk_count += (df_target[col] < 0).astype(int)
            elif col == 'Score_Asset':
                risk_count += (df_target[col] == 0).astype(int)
            else:
                risk_count += (df_target[col] > 0).astype(int)
                
    cond_high_risk = (risk_count >= 1)
    df_target['Target'] = np.where(cond_slopes_decrease & cond_high_risk, 1, 0)

    # 2. Raw Features (ìµœì‹  ë°ì´í„°) ì¶”ì¶œ
    print(" - ê³ ê°ë³„ ìµœì‹  ë°ì´í„° ì¶”ì¶œ ì¤‘...")
    df_features = df_raw.sort_values(by=[COL_ID, COL_DATE]).groupby(COL_ID).last().reset_index()

    # 3. ë³‘í•©
    print(" - ë°ì´í„° ë³‘í•© ì¤‘...")
    if COL_ID not in df_target.columns and df_target.index.name == COL_ID:
        df_target = df_target.reset_index()
        
    df_final = pd.merge(df_features, df_target[[COL_ID, 'Target']], on=COL_ID, how='inner')
    
    print(f"âœ… ìµœì¢… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(df_final)}ëª…")
    print(f" - Target ë¶„í¬:\n{df_final['Target'].value_counts()}")
    
    return df_final

# =============================================================================
# [Model Training Functions] ëª¨ë¸ í•™ìŠµ
# =============================================================================
def run_rf_simulation(data, drop_cols=LEAKAGE_COLS):
    print(f"[Info] Random Forest í•™ìŠµ ì‹œì‘")
    
    # ì „ì²˜ë¦¬
    data_clean = data.replace([np.inf, -np.inf], np.nan).fillna(0)
    targets_to_drop = [COL_ID, 'Target'] + drop_cols
    X_temp = data_clean.drop(columns=targets_to_drop, errors='ignore')
    y = data_clean['Target']
    X = X_temp.select_dtypes(include=['number'])
    
    # Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # í•™ìŠµ
    rf = RandomForestClassifier(
        n_estimators=200, max_depth=10, min_samples_leaf=4,
        random_state=42, class_weight='balanced', n_jobs=-1
    )
    rf.fit(X_train, y_train)
    
    # í‰ê°€
    y_pred = rf.predict(X_test)
    y_prob = rf.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_prob)
    
    print(f"ğŸ“Š RF ê²°ê³¼: Accuracy={acc:.4f}, ROC-AUC={roc:.4f}")
    print(classification_report(y_test, y_pred))
    
    return rf, acc, roc

def run_xgboost_simulation(data, drop_cols=LEAKAGE_COLS):
    print(f"[Info] XGBoost í•™ìŠµ ì‹œì‘")
    
    data_clean = data.replace([np.inf, -np.inf], np.nan).fillna(0)
    targets_to_drop = [COL_ID, 'Target'] + drop_cols
    X_temp = data_clean.drop(columns=targets_to_drop, errors='ignore')
    y = data_clean['Target']
    X = X_temp.select_dtypes(include=['number'])
    
    # ì»¬ëŸ¼ëª… íŠ¹ìˆ˜ë¬¸ì ì œê±°
    regex = re.compile(r"\[|\]|<", re.IGNORECASE)
    X.columns = ["".join(x.split()) for x in X.columns]
    X.columns = [regex.sub("_", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X.columns]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    count_neg = (y_train == 0).sum()
    count_pos = (y_train == 1).sum()
    scale_weight = count_neg / count_pos if count_pos > 0 else 1

    xgb_model = xgb.XGBClassifier(
        n_estimators=200, learning_rate=0.05, max_depth=8,
        scale_pos_weight=scale_weight, random_state=42, n_jobs=-1, tree_method='hist'
    )
    xgb_model.fit(X_train, y_train)
    
    y_pred = xgb_model.predict(X_test)
    y_prob = xgb_model.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_prob)
    
    print(f"ğŸ“Š XGB ê²°ê³¼: Accuracy={acc:.4f}, ROC-AUC={roc:.4f}")
    
    return xgb_model, acc, roc

def run_lightgbm_simulation(data, drop_cols=LEAKAGE_COLS):
    print(f"[Info] LightGBM í•™ìŠµ ì‹œì‘")
    
    data_clean = data.replace([np.inf, -np.inf], np.nan).fillna(0)
    targets_to_drop = [COL_ID, 'Target'] + drop_cols
    X_temp = data_clean.drop(columns=targets_to_drop, errors='ignore')
    y = data_clean['Target']
    X = X_temp.select_dtypes(include=['number'])
    
    regex = re.compile(r"[\[\]<>\s,]", re.IGNORECASE)
    X.columns = [regex.sub("_", str(col)) for col in X.columns]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    count_neg = (y_train == 0).sum()
    count_pos = (y_train == 1).sum()
    scale_weight = count_neg / count_pos if count_pos > 0 else 1

    lgbm_model = lgb.LGBMClassifier(
        n_estimators=200, learning_rate=0.05, max_depth=8, num_leaves=31,
        scale_pos_weight=scale_weight, random_state=42, n_jobs=-1, verbose=-1
    )
    lgbm_model.fit(X_train, y_train)
    
    y_pred = lgbm_model.predict(X_test)
    y_prob = lgbm_model.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_prob)
    
    print(f"ğŸ“Š LGBM ê²°ê³¼: Accuracy={acc:.4f}, ROC-AUC={roc:.4f}")
    
    return lgbm_model, acc, roc

# =============================================================================
# [Visualization] ì‹œê°í™”
# =============================================================================
def compare_existing_models(models_dict, data, drop_cols=LEAKAGE_COLS):
    """í•™ìŠµëœ ëª¨ë¸ë“¤ ë¹„êµ ì‹œê°í™” (ë°ì´í„°ì…‹ì„ ë‹¤ì‹œ Splití•˜ì—¬ ë™ì¼ ì¡°ê±´ í‰ê°€)"""
    print(f"\n[Info] ëª¨ë¸ ë¹„êµ ë¶„ì„ ì‹œì‘...")
    
    # Test set ì¤€ë¹„ (ë™ì¼í•œ Random State ì‚¬ìš©)
    data_clean = data.replace([np.inf, -np.inf], np.nan).fillna(0)
    targets_to_drop = [COL_ID, 'Target'] + drop_cols
    X_temp = data_clean.drop(columns=targets_to_drop, errors='ignore')
    y = data_clean['Target']
    X = X_temp.select_dtypes(include=['number'])
    
    # ì»¬ëŸ¼ëª… ì „ì²˜ë¦¬ (LGBM/XGB í˜¸í™˜)
    regex = re.compile(r"[\[\]<>\s,]", re.IGNORECASE)
    X.columns = [regex.sub("_", str(col)) for col in X.columns]
    
    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    plt.figure(figsize=(10, 6))
    for name, model in models_dict.items():
        if model is None: continue
        
        # ëª¨ë¸ë§ˆë‹¤ feature ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ try-except ì²˜ë¦¬ ë˜ëŠ” ì¬í•™ìŠµ ê¶Œì¥ë˜ë‚˜
        # ì—¬ê¸°ì„œëŠ” ê°™ì€ ìˆœì„œ ì „ì œë¡œ ì§„í–‰
        try:
            y_prob = model.predict_proba(X_test)[:, 1]
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc = roc_auc_score(y_test, y_prob)
            plt.plot(fpr, tpr, label=f"{name} (AUC={roc:.4f})")
        except Exception as e:
            print(f"âš ï¸ {name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")

    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Model ROC Curve Comparison')
    plt.legend()
    plt.show()

# =============================================================================
# [Main] ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================
if __name__ == "__main__":
    # ë°ì´í„° ê²½ë¡œ ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
    TARGET_FILE_PATH = "260108/general_combined_part0.csv" # ì˜ˆì‹œ íŒŒì¼ëª…
    
    if os.path.exists(TARGET_FILE_PATH):
        # 1. ë°ì´í„° ë¡œë“œ ë° ì ìˆ˜ ìƒì„±
        df_viz, df_raw = analyze_and_extract_features_v2(TARGET_FILE_PATH)
        
        if df_viz is not None:
            # 2. ì´íƒˆ ë¶„ì„ (ì˜µì…˜)
            # analyze_rolling_churn(TARGET_FILE_PATH)
            
            # 3. ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°ì…‹ ë³‘í•©
            df_ml = make_ml_dataset_final(df_raw, df_viz)
            
            # 4. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
            rf_model, rf_acc, rf_roc = run_rf_simulation(df_ml)
            xgb_model, xgb_acc, xgb_roc = run_xgboost_simulation(df_ml)
            lgbm_model, lgb_acc, lgb_roc = run_lightgbm_simulation(df_ml)
            
            # 5. ë¹„êµ ì‹œê°í™”
            models = {
                'Random Forest': rf_model,
                'XGBoost': xgb_model,
                'LightGBM': lgbm_model
            }
            compare_existing_models(models, df_ml)
            
            print("\nâœ… ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨")
    else:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {TARGET_FILE_PATH}")
        print("ìŠ¤í¬ë¦½íŠ¸ í•˜ë‹¨ì˜ TARGET_FILE_PATH ë³€ìˆ˜ë¥¼ ì‹¤ì œ ë°ì´í„° ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.")
